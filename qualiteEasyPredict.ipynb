{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.read_csv(\"trvxiadsinit.csv\", sep=\",\", header=0)\n",
    "df=pd.read_csv(\"base2.csv\", sep=\";\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Data columns (total 13 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   aluminium  3276 non-null   object \n",
      " 1   arsenic    3276 non-null   object \n",
      " 2   barium     3276 non-null   object \n",
      " 3   copper     3276 non-null   object \n",
      " 4   iron       3276 non-null   object \n",
      " 5   zinc       3276 non-null   object \n",
      " 6   lead       3276 non-null   object \n",
      " 7   mercury    3276 non-null   float64\n",
      " 8   silver     3276 non-null   object \n",
      " 9   cadmium    3276 non-null   float64\n",
      " 10  radium     3276 non-null   object \n",
      " 11  selenium   3276 non-null   float64\n",
      " 12  is_safe    3276 non-null   int64  \n",
      "dtypes: float64(3), int64(1), object(9)\n",
      "memory usage: 332.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3164 entries, 0 to 3275\n",
      "Data columns (total 13 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   aluminium  3164 non-null   float64\n",
      " 1   arsenic    3164 non-null   float64\n",
      " 2   barium     3164 non-null   float64\n",
      " 3   copper     3164 non-null   float64\n",
      " 4   iron       3164 non-null   float64\n",
      " 5   zinc       3164 non-null   float64\n",
      " 6   lead       3164 non-null   float64\n",
      " 7   mercury    3164 non-null   float64\n",
      " 8   silver     3164 non-null   float64\n",
      " 9   cadmium    3164 non-null   float64\n",
      " 10  radium     3164 non-null   float64\n",
      " 11  selenium   3164 non-null   float64\n",
      " 12  is_safe    3164 non-null   int64  \n",
      "dtypes: float64(12), int64(1)\n",
      "memory usage: 346.1 KB\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    df[column]=pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "\n",
    "df = df.dropna()      \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for column in df.columns:\n",
    "    df[column]=pd.to_numeric(df[column])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aluminium    0\n",
       "arsenic      0\n",
       "barium       0\n",
       "copper       0\n",
       "iron         0\n",
       "zinc         0\n",
       "lead         0\n",
       "mercury      0\n",
       "silver       0\n",
       "cadmium      0\n",
       "radium       0\n",
       "selenium     0\n",
       "is_safe      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23236\\3582406242.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column].mean(), inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aluminium    0\n",
       "arsenic      0\n",
       "barium       0\n",
       "copper       0\n",
       "iron         0\n",
       "zinc         0\n",
       "lead         0\n",
       "mercury      0\n",
       "silver       0\n",
       "cadmium      0\n",
       "radium       0\n",
       "selenium     0\n",
       "is_safe      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in data.columns:\n",
    "    data[column].fillna(data[column].mean(), inplace=True)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazypredict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      aluminium  arsenic  barium  copper  iron  zinc  lead  mercury  silver  \\\n",
      "0          0.09     0.26    0.14    0.11  0.85  0.71  0.18     0.27    0.29   \n",
      "1          0.09     0.04    0.39    0.31  0.57  0.06  0.27     0.63    0.68   \n",
      "2          0.65     0.85    0.24    0.36  0.57  0.09  0.78     0.01    0.17   \n",
      "3          0.78     0.55    0.00    0.59  0.57  0.78  0.25     0.09    0.85   \n",
      "4          0.27     0.46    0.46    0.36  0.57  0.34  0.82     0.13    0.14   \n",
      "...         ...      ...     ...     ...   ...   ...   ...      ...     ...   \n",
      "3271       0.50     0.08    0.92    0.03  0.64  0.24  0.12     0.01    0.60   \n",
      "3272       0.47     0.09    0.53    0.59  0.64  0.42  0.70     0.21    0.50   \n",
      "3273       0.47     0.63    0.27    0.14  0.64  0.07  0.98     0.21    0.86   \n",
      "3274       0.21     0.04    0.14    0.52  0.64  0.29  0.94     0.68    0.95   \n",
      "3275       0.51     0.04    0.53    0.24  0.64  0.22  0.56     0.75    0.12   \n",
      "\n",
      "      cadmium  radium  selenium  \n",
      "0        0.70    0.29      0.74  \n",
      "1        0.81    0.41      0.85  \n",
      "2        0.48    0.07      0.52  \n",
      "3        0.46    0.05      0.50  \n",
      "4        0.37    0.04      0.41  \n",
      "...       ...     ...       ...  \n",
      "3271     0.71    0.31      0.76  \n",
      "3272     0.54    0.21      0.66  \n",
      "3273     0.57    0.22      0.67  \n",
      "3274     0.46    0.32      0.77  \n",
      "3275     0.63    0.26      0.71  \n",
      "\n",
      "[3164 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#df.columns[0:9]\n",
    "X=data.iloc[:,0:12]\n",
    "print(X)\n",
    "y= data[\"is_safe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 31/32 [00:06<00:00,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 911, number of negative: 1462\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 2373, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383902 -> initscore=-0.473018\n",
      "[LightGBM] [Info] Start training from score -0.473018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:06<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "LGBMClassifier                     0.94               0.93     0.93      0.94   \n",
      "BaggingClassifier                  0.89               0.86     0.86      0.88   \n",
      "DecisionTreeClassifier             0.87               0.86     0.86      0.87   \n",
      "RandomForestClassifier             0.88               0.85     0.85      0.87   \n",
      "NuSVC                              0.84               0.79     0.79      0.83   \n",
      "ExtraTreesClassifier               0.83               0.79     0.79      0.82   \n",
      "SVC                                0.83               0.79     0.79      0.82   \n",
      "ExtraTreeClassifier                0.73               0.71     0.71      0.73   \n",
      "SGDClassifier                      0.72               0.69     0.69      0.72   \n",
      "AdaBoostClassifier                 0.73               0.67     0.67      0.71   \n",
      "QuadraticDiscriminantAnalysis      0.74               0.67     0.67      0.70   \n",
      "LogisticRegression                 0.70               0.64     0.64      0.68   \n",
      "KNeighborsClassifier               0.68               0.64     0.64      0.67   \n",
      "CalibratedClassifierCV             0.70               0.64     0.64      0.67   \n",
      "LinearSVC                          0.70               0.64     0.64      0.67   \n",
      "PassiveAggressiveClassifier        0.62               0.64     0.64      0.63   \n",
      "LinearDiscriminantAnalysis         0.70               0.63     0.63      0.67   \n",
      "RidgeClassifier                    0.70               0.63     0.63      0.67   \n",
      "RidgeClassifierCV                  0.70               0.63     0.63      0.67   \n",
      "NearestCentroid                    0.62               0.62     0.62      0.63   \n",
      "LabelPropagation                   0.63               0.61     0.61      0.63   \n",
      "LabelSpreading                     0.63               0.61     0.61      0.63   \n",
      "BernoulliNB                        0.62               0.60     0.60      0.62   \n",
      "Perceptron                         0.65               0.59     0.59      0.63   \n",
      "GaussianNB                         0.65               0.59     0.59      0.63   \n",
      "DummyClassifier                    0.62               0.50     0.50      0.47   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "LGBMClassifier                       0.39  \n",
      "BaggingClassifier                    0.49  \n",
      "DecisionTreeClassifier               0.06  \n",
      "RandomForestClassifier               1.64  \n",
      "NuSVC                                0.43  \n",
      "ExtraTreesClassifier                 0.65  \n",
      "SVC                                  0.37  \n",
      "ExtraTreeClassifier                  0.02  \n",
      "SGDClassifier                        0.03  \n",
      "AdaBoostClassifier                   0.49  \n",
      "QuadraticDiscriminantAnalysis        0.02  \n",
      "LogisticRegression                   0.02  \n",
      "KNeighborsClassifier                 0.21  \n",
      "CalibratedClassifierCV               0.06  \n",
      "LinearSVC                            0.02  \n",
      "PassiveAggressiveClassifier          0.04  \n",
      "LinearDiscriminantAnalysis           0.03  \n",
      "RidgeClassifier                      0.03  \n",
      "RidgeClassifierCV                    0.03  \n",
      "NearestCentroid                      0.02  \n",
      "LabelPropagation                     0.62  \n",
      "LabelSpreading                       0.54  \n",
      "BernoulliNB                          0.03  \n",
      "Perceptron                           0.02  \n",
      "GaussianNB                           0.04  \n",
      "DummyClassifier                      0.02  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25,random_state =123)\n",
    "\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "LGBMClassifier                     0.94               0.93     0.93      0.94   \n",
      "BaggingClassifier                  0.89               0.86     0.86      0.88   \n",
      "DecisionTreeClassifier             0.87               0.86     0.86      0.87   \n",
      "RandomForestClassifier             0.88               0.85     0.85      0.87   \n",
      "NuSVC                              0.84               0.79     0.79      0.83   \n",
      "ExtraTreesClassifier               0.83               0.79     0.79      0.82   \n",
      "SVC                                0.83               0.79     0.79      0.82   \n",
      "ExtraTreeClassifier                0.73               0.71     0.71      0.73   \n",
      "SGDClassifier                      0.72               0.69     0.69      0.72   \n",
      "AdaBoostClassifier                 0.73               0.67     0.67      0.71   \n",
      "QuadraticDiscriminantAnalysis      0.74               0.67     0.67      0.70   \n",
      "LogisticRegression                 0.70               0.64     0.64      0.68   \n",
      "KNeighborsClassifier               0.68               0.64     0.64      0.67   \n",
      "CalibratedClassifierCV             0.70               0.64     0.64      0.67   \n",
      "LinearSVC                          0.70               0.64     0.64      0.67   \n",
      "PassiveAggressiveClassifier        0.62               0.64     0.64      0.63   \n",
      "LinearDiscriminantAnalysis         0.70               0.63     0.63      0.67   \n",
      "RidgeClassifier                    0.70               0.63     0.63      0.67   \n",
      "RidgeClassifierCV                  0.70               0.63     0.63      0.67   \n",
      "NearestCentroid                    0.62               0.62     0.62      0.63   \n",
      "LabelPropagation                   0.63               0.61     0.61      0.63   \n",
      "LabelSpreading                     0.63               0.61     0.61      0.63   \n",
      "BernoulliNB                        0.62               0.60     0.60      0.62   \n",
      "Perceptron                         0.65               0.59     0.59      0.63   \n",
      "GaussianNB                         0.65               0.59     0.59      0.63   \n",
      "DummyClassifier                    0.62               0.50     0.50      0.47   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "LGBMClassifier                       0.39  \n",
      "BaggingClassifier                    0.49  \n",
      "DecisionTreeClassifier               0.06  \n",
      "RandomForestClassifier               1.64  \n",
      "NuSVC                                0.43  \n",
      "ExtraTreesClassifier                 0.65  \n",
      "SVC                                  0.37  \n",
      "ExtraTreeClassifier                  0.02  \n",
      "SGDClassifier                        0.03  \n",
      "AdaBoostClassifier                   0.49  \n",
      "QuadraticDiscriminantAnalysis        0.02  \n",
      "LogisticRegression                   0.02  \n",
      "KNeighborsClassifier                 0.21  \n",
      "CalibratedClassifierCV               0.06  \n",
      "LinearSVC                            0.02  \n",
      "PassiveAggressiveClassifier          0.04  \n",
      "LinearDiscriminantAnalysis           0.03  \n",
      "RidgeClassifier                      0.03  \n",
      "RidgeClassifierCV                    0.03  \n",
      "NearestCentroid                      0.02  \n",
      "LabelPropagation                     0.62  \n",
      "LabelSpreading                       0.54  \n",
      "BernoulliNB                          0.03  \n",
      "Perceptron                           0.02  \n",
      "GaussianNB                           0.04  \n",
      "DummyClassifier                      0.02  \n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle : LGBMClassifier\n",
      "[LightGBM] [Info] Number of positive: 1453, number of negative: 1473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 2926, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496582 -> initscore=-0.013671\n",
      "[LightGBM] [Info] Start training from score -0.013671\n",
      "Entraînement du modèle : BaggingClassifier\n",
      "Entraînement du modèle : RandomForestClassifier\n",
      "Entraînement du modèle : DecisionTreeClassifier\n",
      "Entraînement du modèle : ExtraTreeClassifier\n",
      "Entraînement du modèle : GradientBoostingClassifier\n",
      "Entraînement du modèle : HistGradientBoostingClassifier\n",
      "Entraînement du modèle : AdaBoostClassifier\n",
      "Entraînement du modèle : CatBoostClassifier\n",
      "Résumé des performances :\n",
      "LGBMClassifier: 0.9539\n",
      "BaggingClassifier: 0.9170\n",
      "RandomForestClassifier: 0.9119\n",
      "DecisionTreeClassifier: 0.8924\n",
      "ExtraTreeClassifier: 0.6619\n",
      "GradientBoostingClassifier: 0.8975\n",
      "HistGradientBoostingClassifier: 0.9559\n",
      "AdaBoostClassifier: 0.7152\n",
      "CatBoostClassifier: 0.9221\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "sme= SMOTEENN(random_state=42)\n",
    "# X_res,y_res= sme.fit_resample(X,y)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_res, y_res,test_size=0.25,random_state =123)\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"LGBMClassifier\": LGBMClassifier(),\n",
    "    \"BaggingClassifier\": BaggingClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"ExtraTreeClassifier\": ExtraTreeClassifier(),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "    \"HistGradientBoostingClassifier\": HistGradientBoostingClassifier(),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "    #\"XGBClassifier\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "    \"CatBoostClassifier\": CatBoostClassifier(verbose=0)  # Suppression des logs pour CatBoost\n",
    "}\n",
    "\n",
    "# Entraînement et évaluation des modèles\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Entraînement du modèle : {model_name}\")\n",
    "    model.fit(X_train, y_train)  # Entraînement\n",
    "    y_pred = model.predict(X_test)  # Prédiction\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Évaluation\n",
    "    results[model_name] = accuracy\n",
    "    # print(f\"  → Accuracy : {accuracy:.4f}\\n\")\n",
    "\n",
    "# Résultats finaux\n",
    "print(\"Résumé des performances :\")\n",
    "for model_name, acc in results.items():\n",
    "    print(f\"{model_name}: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrainement des modèles\n",
    "# scikit-learn\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier  \n",
    "# lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# xgboost\n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "# catboost\n",
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sme= SMOTE(random_state=42)\n",
    "X_res,y_res= sme.fit_resample(X,y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res,test_size=0.25,random_state =123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_safe\n",
       "0    1951\n",
       "1    1951\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_res.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entraînement du modèle : LGBMClassifier\n",
      "[LightGBM] [Info] Number of positive: 1453, number of negative: 1473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 2926, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496582 -> initscore=-0.013671\n",
      "[LightGBM] [Info] Start training from score -0.013671\n",
      "Entraînement du modèle : BaggingClassifier\n",
      "Entraînement du modèle : RandomForestClassifier\n",
      "Entraînement du modèle : DecisionTreeClassifier\n",
      "Entraînement du modèle : ExtraTreeClassifier\n",
      "Entraînement du modèle : GradientBoostingClassifier\n",
      "Entraînement du modèle : HistGradientBoostingClassifier\n",
      "Entraînement du modèle : AdaBoostClassifier\n",
      "Entraînement du modèle : XGBClassifier\n",
      "Entraînement du modèle : CatBoostClassifier\n",
      "Résumé des performances :\n",
      "LGBMClassifier: 0.9539\n",
      "BaggingClassifier: 0.9160\n",
      "RandomForestClassifier: 0.9211\n",
      "DecisionTreeClassifier: 0.8934\n",
      "ExtraTreeClassifier: 0.6721\n",
      "GradientBoostingClassifier: 0.8986\n",
      "HistGradientBoostingClassifier: 0.9559\n",
      "AdaBoostClassifier: 0.7152\n",
      "XGBClassifier: 0.9416\n",
      "CatBoostClassifier: 0.9221\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Entraînement du modèle : {model_name}\")\n",
    "    model.fit(X_train, y_train)  # Entraînement\n",
    "    y_pred = model.predict(X_test)  # Prédiction\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # Évaluation\n",
    "    results[model_name] = accuracy\n",
    "    # print(f\"  → Accuracy : {accuracy:.4f}\\n\")\n",
    "\n",
    "# Résultats finaux\n",
    "print(\"Résumé des performances :\")\n",
    "for model_name, acc in results.items():\n",
    "    print(f\"{model_name}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, AdaBoostClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # 🔹 Définition des modèles et de leurs hyperparamètres\n",
    "# param_grids = {\n",
    "#     \"LGBMClassifier\": {\n",
    "#         \"num_leaves\": [20, 31, 40, 50],\n",
    "#         \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "#         \"n_estimators\": [100, 200, 300, 500]\n",
    "#     },\n",
    "#     \"BaggingClassifier\": {\n",
    "#         \"n_estimators\": [10, 50, 100, 200],\n",
    "#         \"max_samples\": [0.5, 0.7, 1.0],\n",
    "#         \"bootstrap\": [True, False]\n",
    "#     },\n",
    "#     \"RandomForestClassifier\": {\n",
    "#         \"n_estimators\": [100, 200, 300, 500],\n",
    "#         \"max_depth\": [10, 20, None],\n",
    "#         \"min_samples_split\": [2, 5, 10]\n",
    "#     },\n",
    "#     \"DecisionTreeClassifier\": {\n",
    "#         \"criterion\": [\"gini\", \"entropy\"],\n",
    "#         \"max_depth\": [5, 10, 20, None],\n",
    "#         \"min_samples_split\": [2, 5, 10]\n",
    "#     },\n",
    "#     \"ExtraTreeClassifier\": {\n",
    "#         \"criterion\": [\"gini\", \"entropy\"],\n",
    "#         \"max_depth\": [5, 10, 20, None],\n",
    "#         \"min_samples_split\": [2, 5, 10]\n",
    "#     },\n",
    "#     \"GradientBoostingClassifier\": {\n",
    "#         \"n_estimators\": [100, 200, 300],\n",
    "#         \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "#         \"max_depth\": [3, 5, 10]\n",
    "#     },\n",
    "#     \"HistGradientBoostingClassifier\": {\n",
    "#         \"max_iter\": [100, 200, 300],\n",
    "#         \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "#         \"max_depth\": [3, 5, 10]\n",
    "#     },\n",
    "#     \"AdaBoostClassifier\": {\n",
    "#         \"n_estimators\": [50, 100, 200, 300],\n",
    "#         \"learning_rate\": [0.01, 0.05, 0.1, 0.2]\n",
    "#     },\n",
    "#     \"XGBClassifier\": {\n",
    "#         \"n_estimators\": [100, 200, 300],\n",
    "#         \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "#         \"max_depth\": [3, 5, 10]\n",
    "#     },\n",
    "#     \"CatBoostClassifier\": {\n",
    "#         \"iterations\": [100, 200, 300],\n",
    "#         \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "#         \"depth\": [3, 5, 10]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # 🔹 Recherche des meilleurs hyperparamètres avec RandomizedSearchCV\n",
    "# best_models = {}\n",
    "# for model_name, model in models.items():\n",
    "#     print(f\"\\n🔍 Optimisation des hyperparamètres pour {model_name}...\")\n",
    "\n",
    "#     param_grid = param_grids.get(model_name, None)\n",
    "#     if param_grid:\n",
    "#         search = RandomizedSearchCV(model, param_grid, n_iter=10, cv=3, scoring=\"accuracy\", random_state=42, n_jobs=-1)\n",
    "#         search.fit(X_train, y_train)\n",
    "        \n",
    "#         # Sauvegarder le meilleur modèle\n",
    "#         best_models[model_name] = search.best_estimator_\n",
    "        \n",
    "#         print(f\"✅ Meilleurs hyperparamètres pour {model_name}: {search.best_params_}\")\n",
    "#         print(f\"✅ Meilleurs hyperparamètres pour {model_name}: {search.best_score_}\")\n",
    "#     else:\n",
    "#         print(f\"⚠️ Pas de grille d'hyperparamètres définie pour {model_name}.\")\n",
    "\n",
    "# # 🔹 Résumé des meilleurs hyperparamètres trouvés\n",
    "# print(\"\\n📊 Résumé des meilleurs hyperparamètres :\")\n",
    "# for model_name, best_model in best_models.items():\n",
    "#     print(f\"{model_name}: {best_model.get_params()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 🔹 Recherche des meilleurs hyperparamètres avec RandomizedSearchCV\n",
    "# best_models = {}\n",
    "# for model_name, model in models.items():\n",
    "#     print(f\"\\n🔍 Optimisation des hyperparamètres pour {model_name}...\")\n",
    "\n",
    "#     param_grid = param_grids.get(model_name, None)\n",
    "#     if param_grid:\n",
    "#         search = RandomizedSearchCV(model, param_grid, n_iter=10, cv=3, scoring=\"accuracy\", random_state=42, n_jobs=-1)\n",
    "#         search.fit(X_res, y_res)\n",
    "        \n",
    "#         # Sauvegarder le meilleur modèle\n",
    "#         best_models[model_name] = search.best_estimator_\n",
    "        \n",
    "#         print(f\"✅ Meilleurs hyperparamètres pour {model_name}: {search.best_params_}\")\n",
    "#     else:\n",
    "#         print(f\"⚠️ Pas de grille d'hyperparamètres définie pour {model_name}.\")\n",
    "\n",
    "# # 🔹 Résumé des meilleurs hyperparamètres trouvés\n",
    "# print(\"\\n📊 Résumé des meilleurs hyperparamètres :\")\n",
    "# for model_name, best_model in best_models.items():\n",
    "#     print(f\"{model_name}: {best_model.get_params()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    \"LGBMClassifier\": {\n",
    "        \"num_leaves\": [20, 31, 40, 50],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "        \"n_estimators\": [100, 200, 300, 500]\n",
    "    },\n",
    "    \"BaggingClassifier\": {\n",
    "        \"n_estimators\": [10, 50, 100, 200],\n",
    "        \"max_samples\": [0.5, 0.7, 1.0],\n",
    "        \"bootstrap\": [True, False]\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"n_estimators\": [100, 200, 300, 500],\n",
    "        \"max_depth\": [10, 20, None],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"DecisionTreeClassifier\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [5, 10, 20, None],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"ExtraTreeClassifier\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [5, 10, 20, None],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"GradientBoostingClassifier\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 5, 10]\n",
    "    },\n",
    "    \"HistGradientBoostingClassifier\": {\n",
    "        \"max_iter\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 5, 10]\n",
    "    },\n",
    "    \"AdaBoostClassifier\": {\n",
    "        \"n_estimators\": [50, 100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2]\n",
    "    },\n",
    "    # \"XGBClassifier\": {\n",
    "    #     \"n_estimators\": [100, 200, 300],\n",
    "    #     \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    #     \"max_depth\": [3, 5, 10]\n",
    "    # },\n",
    "    \"CatBoostClassifier\": {\n",
    "        \"iterations\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "        \"depth\": [3, 5, 10]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimisation des hyperparamètres pour LGBMClassifier...\n",
      "[LightGBM] [Info] Number of positive: 1453, number of negative: 1473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 2926, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496582 -> initscore=-0.013671\n",
      "[LightGBM] [Info] Start training from score -0.013671\n",
      " Meilleurs hyperparamètres pour LGBMClassifier: {'learning_rate': 0.2, 'n_estimators': 100, 'num_leaves': 40}\n",
      " Meilleurs score pour LGBMClassifier: 0.9460004203446827\n",
      "\n",
      " Optimisation des hyperparamètres pour BaggingClassifier...\n",
      " Meilleurs hyperparamètres pour BaggingClassifier: {'bootstrap': True, 'max_samples': 0.7, 'n_estimators': 200}\n",
      " Meilleurs score pour BaggingClassifier: 0.9220772033067114\n",
      "\n",
      " Optimisation des hyperparamètres pour RandomForestClassifier...\n",
      " Meilleurs hyperparamètres pour RandomForestClassifier: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 200}\n",
      " Meilleurs score pour RandomForestClassifier: 0.9012298584839568\n",
      "\n",
      " Optimisation des hyperparamètres pour DecisionTreeClassifier...\n",
      " Meilleurs hyperparamètres pour DecisionTreeClassifier: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2}\n",
      " Meilleurs score pour DecisionTreeClassifier: 0.8978100042034468\n",
      "\n",
      " Optimisation des hyperparamètres pour ExtraTreeClassifier...\n",
      " Meilleurs hyperparamètres pour ExtraTreeClassifier: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 10}\n",
      " Meilleurs score pour ExtraTreeClassifier: 0.6890107888468543\n",
      "\n",
      " Optimisation des hyperparamètres pour GradientBoostingClassifier...\n",
      " Meilleurs hyperparamètres pour GradientBoostingClassifier: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}\n",
      " Meilleurs score pour GradientBoostingClassifier: 0.9302819812246041\n",
      "\n",
      " Optimisation des hyperparamètres pour HistGradientBoostingClassifier...\n",
      " Meilleurs hyperparamètres pour HistGradientBoostingClassifier: {'learning_rate': 0.2, 'max_depth': 10, 'max_iter': 200}\n",
      " Meilleurs score pour HistGradientBoostingClassifier: 0.9377970435757321\n",
      "\n",
      " Optimisation des hyperparamètres pour AdaBoostClassifier...\n",
      " Meilleurs hyperparamètres pour AdaBoostClassifier: {'learning_rate': 0.2, 'n_estimators': 300}\n",
      " Meilleurs score pour AdaBoostClassifier: 0.7395789547428891\n",
      "\n",
      " Optimisation des hyperparamètres pour CatBoostClassifier...\n",
      " Meilleurs hyperparamètres pour CatBoostClassifier: {'depth': 10, 'iterations': 300, 'learning_rate': 0.2}\n",
      " Meilleurs score pour CatBoostClassifier: 0.9248122460417543\n",
      "\n",
      "📊 Résumé des meilleurs hyperparamètres :\n",
      "LGBMClassifier: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.2, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 40, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "BaggingClassifier: {'bootstrap': True, 'bootstrap_features': False, 'estimator': None, 'max_features': 1.0, 'max_samples': 0.7, 'n_estimators': 200, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "RandomForestClassifier: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 5, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "DecisionTreeClassifier: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}\n",
      "ExtraTreeClassifier: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'random'}\n",
      "GradientBoostingClassifier: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.05, 'loss': 'log_loss', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 200, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "HistGradientBoostingClassifier: {'categorical_features': 'from_dtype', 'class_weight': None, 'early_stopping': 'auto', 'interaction_cst': None, 'l2_regularization': 0.0, 'learning_rate': 0.2, 'loss': 'log_loss', 'max_bins': 255, 'max_depth': 10, 'max_features': 1.0, 'max_iter': 200, 'max_leaf_nodes': 31, 'min_samples_leaf': 20, 'monotonic_cst': None, 'n_iter_no_change': 10, 'random_state': None, 'scoring': 'loss', 'tol': 1e-07, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "AdaBoostClassifier: {'algorithm': 'deprecated', 'estimator': None, 'learning_rate': 0.2, 'n_estimators': 300, 'random_state': None}\n",
      "CatBoostClassifier: {'verbose': 0, 'depth': 10, 'iterations': 300, 'learning_rate': 0.2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "best_models = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n Optimisation des hyperparamètres pour {model_name}...\")\n",
    "\n",
    "    param_grid = param_grids.get(model_name, None)\n",
    "    if param_grid:\n",
    "        search = GridSearchCV(model, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        # Sauvegarder le meilleur modèle\n",
    "        best_models[model_name] = search.best_estimator_\n",
    "        \n",
    "        print(f\" Meilleurs hyperparamètres pour {model_name}: {search.best_params_}\")\n",
    "        print(f\" Meilleurs score pour {model_name}: {search.best_score_}\")\n",
    "    else:\n",
    "        print(f\" Pas de grille d'hyperparamètres définie pour {model_name}.\")\n",
    "\n",
    "print(\"\\n📊 Résumé des meilleurs hyperparamètres :\")\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f\"{model_name}: {best_model.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimisation des hyperparamètres pour LGBMClassifier...\n",
      "[LightGBM] [Info] Number of positive: 1951, number of negative: 1951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 3902, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      " Meilleurs hyperparamètres pour LGBMClassifier: {'learning_rate': 0.2, 'n_estimators': 500, 'num_leaves': 20}\n",
      " Meilleurs score pour LGBMClassifier: 0.9500305484932695\n",
      "\n",
      " Optimisation des hyperparamètres pour BaggingClassifier...\n",
      " Meilleurs hyperparamètres pour BaggingClassifier: {'bootstrap': True, 'max_samples': 1.0, 'n_estimators': 200}\n",
      " Meilleurs score pour BaggingClassifier: 0.9343964208991111\n",
      "\n",
      " Optimisation des hyperparamètres pour RandomForestClassifier...\n",
      " Meilleurs hyperparamètres pour RandomForestClassifier: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 300}\n",
      " Meilleurs score pour RandomForestClassifier: 0.91184887364749\n",
      "\n",
      " Optimisation des hyperparamètres pour DecisionTreeClassifier...\n",
      " Meilleurs hyperparamètres pour DecisionTreeClassifier: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 2}\n",
      " Meilleurs score pour DecisionTreeClassifier: 0.9026188927649343\n",
      "\n",
      " Optimisation des hyperparamètres pour ExtraTreeClassifier...\n",
      " Meilleurs hyperparamètres pour ExtraTreeClassifier: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10}\n",
      " Meilleurs score pour ExtraTreeClassifier: 0.7216912040048089\n",
      "\n",
      " Optimisation des hyperparamètres pour GradientBoostingClassifier...\n",
      " Meilleurs hyperparamètres pour GradientBoostingClassifier: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 300}\n",
      " Meilleurs score pour GradientBoostingClassifier: 0.9397790654131931\n",
      "\n",
      " Optimisation des hyperparamètres pour HistGradientBoostingClassifier...\n",
      " Meilleurs hyperparamètres pour HistGradientBoostingClassifier: {'learning_rate': 0.2, 'max_depth': 10, 'max_iter': 300}\n",
      " Meilleurs score pour HistGradientBoostingClassifier: 0.9454183172707386\n",
      "\n",
      " Optimisation des hyperparamètres pour AdaBoostClassifier...\n",
      " Meilleurs hyperparamètres pour AdaBoostClassifier: {'learning_rate': 0.2, 'n_estimators': 300}\n",
      " Meilleurs score pour AdaBoostClassifier: 0.7265488086087624\n",
      "\n",
      " Optimisation des hyperparamètres pour CatBoostClassifier...\n",
      " Meilleurs hyperparamètres pour CatBoostClassifier: {'depth': 10, 'iterations': 300, 'learning_rate': 0.2}\n",
      " Meilleurs score pour CatBoostClassifier: 0.9372191016772108\n",
      "\n",
      "📊 Résumé des meilleurs hyperparamètres :\n",
      "LGBMClassifier: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.2, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': None, 'num_leaves': 20, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "BaggingClassifier: {'bootstrap': True, 'bootstrap_features': False, 'estimator': None, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 200, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "RandomForestClassifier: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 300, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "DecisionTreeClassifier: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 20, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}\n",
      "ExtraTreeClassifier: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'random'}\n",
      "GradientBoostingClassifier: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 300, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "HistGradientBoostingClassifier: {'categorical_features': 'from_dtype', 'class_weight': None, 'early_stopping': 'auto', 'interaction_cst': None, 'l2_regularization': 0.0, 'learning_rate': 0.2, 'loss': 'log_loss', 'max_bins': 255, 'max_depth': 10, 'max_features': 1.0, 'max_iter': 300, 'max_leaf_nodes': 31, 'min_samples_leaf': 20, 'monotonic_cst': None, 'n_iter_no_change': 10, 'random_state': None, 'scoring': 'loss', 'tol': 1e-07, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "AdaBoostClassifier: {'algorithm': 'deprecated', 'estimator': None, 'learning_rate': 0.2, 'n_estimators': 300, 'random_state': None}\n",
      "CatBoostClassifier: {'verbose': 0, 'depth': 10, 'iterations': 300, 'learning_rate': 0.2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# 🔹 Recherche des meilleurs hyperparamètres avec GridSearchCV\n",
    "best_models = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n Optimisation des hyperparamètres pour {model_name}...\")\n",
    "\n",
    "    param_grid = param_grids.get(model_name, None)\n",
    "    if param_grid:\n",
    "        search = GridSearchCV(model, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "        search.fit(X_res, y_res)\n",
    "        \n",
    "        # Sauvegarder le meilleur modèle\n",
    "        best_models[model_name] = search.best_estimator_\n",
    "        \n",
    "        print(f\" Meilleurs hyperparamètres pour {model_name}: {search.best_params_}\")\n",
    "        print(f\" Meilleurs score pour {model_name}: {search.best_score_}\")\n",
    "    else:\n",
    "        print(f\" Pas de grille d'hyperparamètres définie pour {model_name}.\")\n",
    "\n",
    "# 🔹 Résumé des meilleurs hyperparamètres trouvés\n",
    "print(\"\\n📊 Résumé des meilleurs hyperparamètres :\")\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f\"{model_name}: {best_model.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1453, number of negative: 1473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 2926, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496582 -> initscore=-0.013671\n",
      "[LightGBM] [Info] Start training from score -0.013671\n",
      "Score du modèle Light: 0.9539\n"
     ]
    }
   ],
   "source": [
    "#Lightgbm\n",
    "model=LGBMClassifier(n_estimators=300, learning_rate= 0.2,  num_leaves= 20)\n",
    "\n",
    "\n",
    "#search = GridSearchCV(model, param_grid=param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Sauvegarder le meilleur modèle\n",
    "best_models[model_name] = search.best_estimator_\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Score du modèle Light: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du modèle Hist: 0.9508\n"
     ]
    }
   ],
   "source": [
    "#Hist\n",
    "\n",
    "\n",
    "#Lightgbm\n",
    "model=HistGradientBoostingClassifier( learning_rate= 0.2, max_depth= 10, max_iter =200)\n",
    "\n",
    "\n",
    "#search = GridSearchCV(model, param_grid=param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Sauvegarder le meilleur modèle\n",
    "best_models[model_name] = search.best_estimator_\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Score du modèle Hist: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
