{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=pd.read_csv(\"trvxiadsinit.csv\", sep=\",\", header=0)\n",
    "df=pd.read_csv(\"base2.csv\", sep=\";\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3276 entries, 0 to 3275\n",
      "Data columns (total 13 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   aluminium  3276 non-null   object \n",
      " 1   arsenic    3276 non-null   object \n",
      " 2   barium     3276 non-null   object \n",
      " 3   copper     3276 non-null   object \n",
      " 4   iron       3276 non-null   object \n",
      " 5   zinc       3276 non-null   object \n",
      " 6   lead       3276 non-null   object \n",
      " 7   mercury    3276 non-null   float64\n",
      " 8   silver     3276 non-null   object \n",
      " 9   cadmium    3276 non-null   float64\n",
      " 10  radium     3276 non-null   object \n",
      " 11  selenium   3276 non-null   float64\n",
      " 12  is_safe    3276 non-null   int64  \n",
      "dtypes: float64(3), int64(1), object(9)\n",
      "memory usage: 332.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3164 entries, 0 to 3275\n",
      "Data columns (total 13 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   aluminium  3164 non-null   float64\n",
      " 1   arsenic    3164 non-null   float64\n",
      " 2   barium     3164 non-null   float64\n",
      " 3   copper     3164 non-null   float64\n",
      " 4   iron       3164 non-null   float64\n",
      " 5   zinc       3164 non-null   float64\n",
      " 6   lead       3164 non-null   float64\n",
      " 7   mercury    3164 non-null   float64\n",
      " 8   silver     3164 non-null   float64\n",
      " 9   cadmium    3164 non-null   float64\n",
      " 10  radium     3164 non-null   float64\n",
      " 11  selenium   3164 non-null   float64\n",
      " 12  is_safe    3164 non-null   int64  \n",
      "dtypes: float64(12), int64(1)\n",
      "memory usage: 346.1 KB\n"
     ]
    }
   ],
   "source": [
    "for column in df.columns:\n",
    "    df[column]=pd.to_numeric(df[column], errors='coerce')\n",
    "\n",
    "\n",
    "df = df.dropna()      \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for column in df.columns:\n",
    "    df[column]=pd.to_numeric(df[column])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aluminium    0\n",
       "arsenic      0\n",
       "barium       0\n",
       "copper       0\n",
       "iron         0\n",
       "zinc         0\n",
       "lead         0\n",
       "mercury      0\n",
       "silver       0\n",
       "cadmium      0\n",
       "radium       0\n",
       "selenium     0\n",
       "is_safe      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23236\\3582406242.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[column].fillna(data[column].mean(), inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "aluminium    0\n",
       "arsenic      0\n",
       "barium       0\n",
       "copper       0\n",
       "iron         0\n",
       "zinc         0\n",
       "lead         0\n",
       "mercury      0\n",
       "silver       0\n",
       "cadmium      0\n",
       "radium       0\n",
       "selenium     0\n",
       "is_safe      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for column in data.columns:\n",
    "    data[column].fillna(data[column].mean(), inplace=True)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lazypredict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      aluminium  arsenic  barium  copper  iron  zinc  lead  mercury  silver  \\\n",
      "0          0.09     0.26    0.14    0.11  0.85  0.71  0.18     0.27    0.29   \n",
      "1          0.09     0.04    0.39    0.31  0.57  0.06  0.27     0.63    0.68   \n",
      "2          0.65     0.85    0.24    0.36  0.57  0.09  0.78     0.01    0.17   \n",
      "3          0.78     0.55    0.00    0.59  0.57  0.78  0.25     0.09    0.85   \n",
      "4          0.27     0.46    0.46    0.36  0.57  0.34  0.82     0.13    0.14   \n",
      "...         ...      ...     ...     ...   ...   ...   ...      ...     ...   \n",
      "3271       0.50     0.08    0.92    0.03  0.64  0.24  0.12     0.01    0.60   \n",
      "3272       0.47     0.09    0.53    0.59  0.64  0.42  0.70     0.21    0.50   \n",
      "3273       0.47     0.63    0.27    0.14  0.64  0.07  0.98     0.21    0.86   \n",
      "3274       0.21     0.04    0.14    0.52  0.64  0.29  0.94     0.68    0.95   \n",
      "3275       0.51     0.04    0.53    0.24  0.64  0.22  0.56     0.75    0.12   \n",
      "\n",
      "      cadmium  radium  selenium  \n",
      "0        0.70    0.29      0.74  \n",
      "1        0.81    0.41      0.85  \n",
      "2        0.48    0.07      0.52  \n",
      "3        0.46    0.05      0.50  \n",
      "4        0.37    0.04      0.41  \n",
      "...       ...     ...       ...  \n",
      "3271     0.71    0.31      0.76  \n",
      "3272     0.54    0.21      0.66  \n",
      "3273     0.57    0.22      0.67  \n",
      "3274     0.46    0.32      0.77  \n",
      "3275     0.63    0.26      0.71  \n",
      "\n",
      "[3164 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#df.columns[0:9]\n",
    "X=data.iloc[:,0:12]\n",
    "print(X)\n",
    "y= data[\"is_safe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 31/32 [00:06<00:00,  5.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 911, number of negative: 1462\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000431 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 2373, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.383902 -> initscore=-0.473018\n",
      "[LightGBM] [Info] Start training from score -0.473018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:06<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "LGBMClassifier                     0.94               0.93     0.93      0.94   \n",
      "BaggingClassifier                  0.89               0.86     0.86      0.88   \n",
      "DecisionTreeClassifier             0.87               0.86     0.86      0.87   \n",
      "RandomForestClassifier             0.88               0.85     0.85      0.87   \n",
      "NuSVC                              0.84               0.79     0.79      0.83   \n",
      "ExtraTreesClassifier               0.83               0.79     0.79      0.82   \n",
      "SVC                                0.83               0.79     0.79      0.82   \n",
      "ExtraTreeClassifier                0.73               0.71     0.71      0.73   \n",
      "SGDClassifier                      0.72               0.69     0.69      0.72   \n",
      "AdaBoostClassifier                 0.73               0.67     0.67      0.71   \n",
      "QuadraticDiscriminantAnalysis      0.74               0.67     0.67      0.70   \n",
      "LogisticRegression                 0.70               0.64     0.64      0.68   \n",
      "KNeighborsClassifier               0.68               0.64     0.64      0.67   \n",
      "CalibratedClassifierCV             0.70               0.64     0.64      0.67   \n",
      "LinearSVC                          0.70               0.64     0.64      0.67   \n",
      "PassiveAggressiveClassifier        0.62               0.64     0.64      0.63   \n",
      "LinearDiscriminantAnalysis         0.70               0.63     0.63      0.67   \n",
      "RidgeClassifier                    0.70               0.63     0.63      0.67   \n",
      "RidgeClassifierCV                  0.70               0.63     0.63      0.67   \n",
      "NearestCentroid                    0.62               0.62     0.62      0.63   \n",
      "LabelPropagation                   0.63               0.61     0.61      0.63   \n",
      "LabelSpreading                     0.63               0.61     0.61      0.63   \n",
      "BernoulliNB                        0.62               0.60     0.60      0.62   \n",
      "Perceptron                         0.65               0.59     0.59      0.63   \n",
      "GaussianNB                         0.65               0.59     0.59      0.63   \n",
      "DummyClassifier                    0.62               0.50     0.50      0.47   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "LGBMClassifier                       0.39  \n",
      "BaggingClassifier                    0.49  \n",
      "DecisionTreeClassifier               0.06  \n",
      "RandomForestClassifier               1.64  \n",
      "NuSVC                                0.43  \n",
      "ExtraTreesClassifier                 0.65  \n",
      "SVC                                  0.37  \n",
      "ExtraTreeClassifier                  0.02  \n",
      "SGDClassifier                        0.03  \n",
      "AdaBoostClassifier                   0.49  \n",
      "QuadraticDiscriminantAnalysis        0.02  \n",
      "LogisticRegression                   0.02  \n",
      "KNeighborsClassifier                 0.21  \n",
      "CalibratedClassifierCV               0.06  \n",
      "LinearSVC                            0.02  \n",
      "PassiveAggressiveClassifier          0.04  \n",
      "LinearDiscriminantAnalysis           0.03  \n",
      "RidgeClassifier                      0.03  \n",
      "RidgeClassifierCV                    0.03  \n",
      "NearestCentroid                      0.02  \n",
      "LabelPropagation                     0.62  \n",
      "LabelSpreading                       0.54  \n",
      "BernoulliNB                          0.03  \n",
      "Perceptron                           0.02  \n",
      "GaussianNB                           0.04  \n",
      "DummyClassifier                      0.02  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25,random_state =123)\n",
    "\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Accuracy  Balanced Accuracy  ROC AUC  F1 Score  \\\n",
      "Model                                                                           \n",
      "LGBMClassifier                     0.94               0.93     0.93      0.94   \n",
      "BaggingClassifier                  0.89               0.86     0.86      0.88   \n",
      "DecisionTreeClassifier             0.87               0.86     0.86      0.87   \n",
      "RandomForestClassifier             0.88               0.85     0.85      0.87   \n",
      "NuSVC                              0.84               0.79     0.79      0.83   \n",
      "ExtraTreesClassifier               0.83               0.79     0.79      0.82   \n",
      "SVC                                0.83               0.79     0.79      0.82   \n",
      "ExtraTreeClassifier                0.73               0.71     0.71      0.73   \n",
      "SGDClassifier                      0.72               0.69     0.69      0.72   \n",
      "AdaBoostClassifier                 0.73               0.67     0.67      0.71   \n",
      "QuadraticDiscriminantAnalysis      0.74               0.67     0.67      0.70   \n",
      "LogisticRegression                 0.70               0.64     0.64      0.68   \n",
      "KNeighborsClassifier               0.68               0.64     0.64      0.67   \n",
      "CalibratedClassifierCV             0.70               0.64     0.64      0.67   \n",
      "LinearSVC                          0.70               0.64     0.64      0.67   \n",
      "PassiveAggressiveClassifier        0.62               0.64     0.64      0.63   \n",
      "LinearDiscriminantAnalysis         0.70               0.63     0.63      0.67   \n",
      "RidgeClassifier                    0.70               0.63     0.63      0.67   \n",
      "RidgeClassifierCV                  0.70               0.63     0.63      0.67   \n",
      "NearestCentroid                    0.62               0.62     0.62      0.63   \n",
      "LabelPropagation                   0.63               0.61     0.61      0.63   \n",
      "LabelSpreading                     0.63               0.61     0.61      0.63   \n",
      "BernoulliNB                        0.62               0.60     0.60      0.62   \n",
      "Perceptron                         0.65               0.59     0.59      0.63   \n",
      "GaussianNB                         0.65               0.59     0.59      0.63   \n",
      "DummyClassifier                    0.62               0.50     0.50      0.47   \n",
      "\n",
      "                               Time Taken  \n",
      "Model                                      \n",
      "LGBMClassifier                       0.39  \n",
      "BaggingClassifier                    0.49  \n",
      "DecisionTreeClassifier               0.06  \n",
      "RandomForestClassifier               1.64  \n",
      "NuSVC                                0.43  \n",
      "ExtraTreesClassifier                 0.65  \n",
      "SVC                                  0.37  \n",
      "ExtraTreeClassifier                  0.02  \n",
      "SGDClassifier                        0.03  \n",
      "AdaBoostClassifier                   0.49  \n",
      "QuadraticDiscriminantAnalysis        0.02  \n",
      "LogisticRegression                   0.02  \n",
      "KNeighborsClassifier                 0.21  \n",
      "CalibratedClassifierCV               0.06  \n",
      "LinearSVC                            0.02  \n",
      "PassiveAggressiveClassifier          0.04  \n",
      "LinearDiscriminantAnalysis           0.03  \n",
      "RidgeClassifier                      0.03  \n",
      "RidgeClassifierCV                    0.03  \n",
      "NearestCentroid                      0.02  \n",
      "LabelPropagation                     0.62  \n",
      "LabelSpreading                       0.54  \n",
      "BernoulliNB                          0.03  \n",
      "Perceptron                           0.02  \n",
      "GaussianNB                           0.04  \n",
      "DummyClassifier                      0.02  \n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entra√Ænement du mod√®le : LGBMClassifier\n",
      "[LightGBM] [Info] Number of positive: 1453, number of negative: 1473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000567 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 2926, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496582 -> initscore=-0.013671\n",
      "[LightGBM] [Info] Start training from score -0.013671\n",
      "Entra√Ænement du mod√®le : BaggingClassifier\n",
      "Entra√Ænement du mod√®le : RandomForestClassifier\n",
      "Entra√Ænement du mod√®le : DecisionTreeClassifier\n",
      "Entra√Ænement du mod√®le : ExtraTreeClassifier\n",
      "Entra√Ænement du mod√®le : GradientBoostingClassifier\n",
      "Entra√Ænement du mod√®le : HistGradientBoostingClassifier\n",
      "Entra√Ænement du mod√®le : AdaBoostClassifier\n",
      "Entra√Ænement du mod√®le : CatBoostClassifier\n",
      "R√©sum√© des performances :\n",
      "LGBMClassifier: 0.9539\n",
      "BaggingClassifier: 0.9170\n",
      "RandomForestClassifier: 0.9119\n",
      "DecisionTreeClassifier: 0.8924\n",
      "ExtraTreeClassifier: 0.6619\n",
      "GradientBoostingClassifier: 0.8975\n",
      "HistGradientBoostingClassifier: 0.9559\n",
      "AdaBoostClassifier: 0.7152\n",
      "CatBoostClassifier: 0.9221\n"
     ]
    }
   ],
   "source": [
    "# Importation des biblioth√®ques n√©cessaires\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "sme= SMOTEENN(random_state=42)\n",
    "# X_res,y_res= sme.fit_resample(X,y)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_res, y_res,test_size=0.25,random_state =123)\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"LGBMClassifier\": LGBMClassifier(),\n",
    "    \"BaggingClassifier\": BaggingClassifier(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
    "    \"ExtraTreeClassifier\": ExtraTreeClassifier(),\n",
    "    \"GradientBoostingClassifier\": GradientBoostingClassifier(),\n",
    "    \"HistGradientBoostingClassifier\": HistGradientBoostingClassifier(),\n",
    "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
    "    #\"XGBClassifier\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\"),\n",
    "    \"CatBoostClassifier\": CatBoostClassifier(verbose=0)  # Suppression des logs pour CatBoost\n",
    "}\n",
    "\n",
    "# Entra√Ænement et √©valuation des mod√®les\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Entra√Ænement du mod√®le : {model_name}\")\n",
    "    model.fit(X_train, y_train)  # Entra√Ænement\n",
    "    y_pred = model.predict(X_test)  # Pr√©diction\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # √âvaluation\n",
    "    results[model_name] = accuracy\n",
    "    # print(f\"  ‚Üí Accuracy : {accuracy:.4f}\\n\")\n",
    "\n",
    "# R√©sultats finaux\n",
    "print(\"R√©sum√© des performances :\")\n",
    "for model_name, acc in results.items():\n",
    "    print(f\"{model_name}: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrainement des mod√®les\n",
    "# scikit-learn\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier  \n",
    "# lightgbm\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# xgboost\n",
    "from xgboost import XGBClassifier \n",
    "\n",
    "# catboost\n",
    "from catboost import CatBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sme= SMOTE(random_state=42)\n",
    "X_res,y_res= sme.fit_resample(X,y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res,test_size=0.25,random_state =123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_safe\n",
       "0    1951\n",
       "1    1951\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y_res.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entra√Ænement du mod√®le : LGBMClassifier\n",
      "[LightGBM] [Info] Number of positive: 1453, number of negative: 1473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000346 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 2926, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496582 -> initscore=-0.013671\n",
      "[LightGBM] [Info] Start training from score -0.013671\n",
      "Entra√Ænement du mod√®le : BaggingClassifier\n",
      "Entra√Ænement du mod√®le : RandomForestClassifier\n",
      "Entra√Ænement du mod√®le : DecisionTreeClassifier\n",
      "Entra√Ænement du mod√®le : ExtraTreeClassifier\n",
      "Entra√Ænement du mod√®le : GradientBoostingClassifier\n",
      "Entra√Ænement du mod√®le : HistGradientBoostingClassifier\n",
      "Entra√Ænement du mod√®le : AdaBoostClassifier\n",
      "Entra√Ænement du mod√®le : XGBClassifier\n",
      "Entra√Ænement du mod√®le : CatBoostClassifier\n",
      "R√©sum√© des performances :\n",
      "LGBMClassifier: 0.9539\n",
      "BaggingClassifier: 0.9160\n",
      "RandomForestClassifier: 0.9211\n",
      "DecisionTreeClassifier: 0.8934\n",
      "ExtraTreeClassifier: 0.6721\n",
      "GradientBoostingClassifier: 0.8986\n",
      "HistGradientBoostingClassifier: 0.9559\n",
      "AdaBoostClassifier: 0.7152\n",
      "XGBClassifier: 0.9416\n",
      "CatBoostClassifier: 0.9221\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Entra√Ænement du mod√®le : {model_name}\")\n",
    "    model.fit(X_train, y_train)  # Entra√Ænement\n",
    "    y_pred = model.predict(X_test)  # Pr√©diction\n",
    "    accuracy = accuracy_score(y_test, y_pred)  # √âvaluation\n",
    "    results[model_name] = accuracy\n",
    "    # print(f\"  ‚Üí Accuracy : {accuracy:.4f}\\n\")\n",
    "\n",
    "# R√©sultats finaux\n",
    "print(\"R√©sum√© des performances :\")\n",
    "for model_name, acc in results.items():\n",
    "    print(f\"{model_name}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier, AdaBoostClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "# from lightgbm import LGBMClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # üîπ D√©finition des mod√®les et de leurs hyperparam√®tres\n",
    "# param_grids = {\n",
    "#     \"LGBMClassifier\": {\n",
    "#         \"num_leaves\": [20, 31, 40, 50],\n",
    "#         \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "#         \"n_estimators\": [100, 200, 300, 500]\n",
    "#     },\n",
    "#     \"BaggingClassifier\": {\n",
    "#         \"n_estimators\": [10, 50, 100, 200],\n",
    "#         \"max_samples\": [0.5, 0.7, 1.0],\n",
    "#         \"bootstrap\": [True, False]\n",
    "#     },\n",
    "#     \"RandomForestClassifier\": {\n",
    "#         \"n_estimators\": [100, 200, 300, 500],\n",
    "#         \"max_depth\": [10, 20, None],\n",
    "#         \"min_samples_split\": [2, 5, 10]\n",
    "#     },\n",
    "#     \"DecisionTreeClassifier\": {\n",
    "#         \"criterion\": [\"gini\", \"entropy\"],\n",
    "#         \"max_depth\": [5, 10, 20, None],\n",
    "#         \"min_samples_split\": [2, 5, 10]\n",
    "#     },\n",
    "#     \"ExtraTreeClassifier\": {\n",
    "#         \"criterion\": [\"gini\", \"entropy\"],\n",
    "#         \"max_depth\": [5, 10, 20, None],\n",
    "#         \"min_samples_split\": [2, 5, 10]\n",
    "#     },\n",
    "#     \"GradientBoostingClassifier\": {\n",
    "#         \"n_estimators\": [100, 200, 300],\n",
    "#         \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "#         \"max_depth\": [3, 5, 10]\n",
    "#     },\n",
    "#     \"HistGradientBoostingClassifier\": {\n",
    "#         \"max_iter\": [100, 200, 300],\n",
    "#         \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "#         \"max_depth\": [3, 5, 10]\n",
    "#     },\n",
    "#     \"AdaBoostClassifier\": {\n",
    "#         \"n_estimators\": [50, 100, 200, 300],\n",
    "#         \"learning_rate\": [0.01, 0.05, 0.1, 0.2]\n",
    "#     },\n",
    "#     \"XGBClassifier\": {\n",
    "#         \"n_estimators\": [100, 200, 300],\n",
    "#         \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "#         \"max_depth\": [3, 5, 10]\n",
    "#     },\n",
    "#     \"CatBoostClassifier\": {\n",
    "#         \"iterations\": [100, 200, 300],\n",
    "#         \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "#         \"depth\": [3, 5, 10]\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # üîπ Recherche des meilleurs hyperparam√®tres avec RandomizedSearchCV\n",
    "# best_models = {}\n",
    "# for model_name, model in models.items():\n",
    "#     print(f\"\\nüîç Optimisation des hyperparam√®tres pour {model_name}...\")\n",
    "\n",
    "#     param_grid = param_grids.get(model_name, None)\n",
    "#     if param_grid:\n",
    "#         search = RandomizedSearchCV(model, param_grid, n_iter=10, cv=3, scoring=\"accuracy\", random_state=42, n_jobs=-1)\n",
    "#         search.fit(X_train, y_train)\n",
    "        \n",
    "#         # Sauvegarder le meilleur mod√®le\n",
    "#         best_models[model_name] = search.best_estimator_\n",
    "        \n",
    "#         print(f\"‚úÖ Meilleurs hyperparam√®tres pour {model_name}: {search.best_params_}\")\n",
    "#         print(f\"‚úÖ Meilleurs hyperparam√®tres pour {model_name}: {search.best_score_}\")\n",
    "#     else:\n",
    "#         print(f\"‚ö†Ô∏è Pas de grille d'hyperparam√®tres d√©finie pour {model_name}.\")\n",
    "\n",
    "# # üîπ R√©sum√© des meilleurs hyperparam√®tres trouv√©s\n",
    "# print(\"\\nüìä R√©sum√© des meilleurs hyperparam√®tres :\")\n",
    "# for model_name, best_model in best_models.items():\n",
    "#     print(f\"{model_name}: {best_model.get_params()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # üîπ Recherche des meilleurs hyperparam√®tres avec RandomizedSearchCV\n",
    "# best_models = {}\n",
    "# for model_name, model in models.items():\n",
    "#     print(f\"\\nüîç Optimisation des hyperparam√®tres pour {model_name}...\")\n",
    "\n",
    "#     param_grid = param_grids.get(model_name, None)\n",
    "#     if param_grid:\n",
    "#         search = RandomizedSearchCV(model, param_grid, n_iter=10, cv=3, scoring=\"accuracy\", random_state=42, n_jobs=-1)\n",
    "#         search.fit(X_res, y_res)\n",
    "        \n",
    "#         # Sauvegarder le meilleur mod√®le\n",
    "#         best_models[model_name] = search.best_estimator_\n",
    "        \n",
    "#         print(f\"‚úÖ Meilleurs hyperparam√®tres pour {model_name}: {search.best_params_}\")\n",
    "#     else:\n",
    "#         print(f\"‚ö†Ô∏è Pas de grille d'hyperparam√®tres d√©finie pour {model_name}.\")\n",
    "\n",
    "# # üîπ R√©sum√© des meilleurs hyperparam√®tres trouv√©s\n",
    "# print(\"\\nüìä R√©sum√© des meilleurs hyperparam√®tres :\")\n",
    "# for model_name, best_model in best_models.items():\n",
    "#     print(f\"{model_name}: {best_model.get_params()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    \"LGBMClassifier\": {\n",
    "        \"num_leaves\": [20, 31, 40, 50],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "        \"n_estimators\": [100, 200, 300, 500]\n",
    "    },\n",
    "    \"BaggingClassifier\": {\n",
    "        \"n_estimators\": [10, 50, 100, 200],\n",
    "        \"max_samples\": [0.5, 0.7, 1.0],\n",
    "        \"bootstrap\": [True, False]\n",
    "    },\n",
    "    \"RandomForestClassifier\": {\n",
    "        \"n_estimators\": [100, 200, 300, 500],\n",
    "        \"max_depth\": [10, 20, None],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"DecisionTreeClassifier\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [5, 10, 20, None],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"ExtraTreeClassifier\": {\n",
    "        \"criterion\": [\"gini\", \"entropy\"],\n",
    "        \"max_depth\": [5, 10, 20, None],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"GradientBoostingClassifier\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 5, 10]\n",
    "    },\n",
    "    \"HistGradientBoostingClassifier\": {\n",
    "        \"max_iter\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 5, 10]\n",
    "    },\n",
    "    \"AdaBoostClassifier\": {\n",
    "        \"n_estimators\": [50, 100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2]\n",
    "    },\n",
    "    # \"XGBClassifier\": {\n",
    "    #     \"n_estimators\": [100, 200, 300],\n",
    "    #     \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    #     \"max_depth\": [3, 5, 10]\n",
    "    # },\n",
    "    \"CatBoostClassifier\": {\n",
    "        \"iterations\": [100, 200, 300],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "        \"depth\": [3, 5, 10]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimisation des hyperparam√®tres pour LGBMClassifier...\n",
      "[LightGBM] [Info] Number of positive: 1453, number of negative: 1473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000580 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 2926, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496582 -> initscore=-0.013671\n",
      "[LightGBM] [Info] Start training from score -0.013671\n",
      " Meilleurs hyperparam√®tres pour LGBMClassifier: {'learning_rate': 0.2, 'n_estimators': 100, 'num_leaves': 40}\n",
      " Meilleurs score pour LGBMClassifier: 0.9460004203446827\n",
      "\n",
      " Optimisation des hyperparam√®tres pour BaggingClassifier...\n",
      " Meilleurs hyperparam√®tres pour BaggingClassifier: {'bootstrap': True, 'max_samples': 0.7, 'n_estimators': 200}\n",
      " Meilleurs score pour BaggingClassifier: 0.9220772033067114\n",
      "\n",
      " Optimisation des hyperparam√®tres pour RandomForestClassifier...\n",
      " Meilleurs hyperparam√®tres pour RandomForestClassifier: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 200}\n",
      " Meilleurs score pour RandomForestClassifier: 0.9012298584839568\n",
      "\n",
      " Optimisation des hyperparam√®tres pour DecisionTreeClassifier...\n",
      " Meilleurs hyperparam√®tres pour DecisionTreeClassifier: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 2}\n",
      " Meilleurs score pour DecisionTreeClassifier: 0.8978100042034468\n",
      "\n",
      " Optimisation des hyperparam√®tres pour ExtraTreeClassifier...\n",
      " Meilleurs hyperparam√®tres pour ExtraTreeClassifier: {'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 10}\n",
      " Meilleurs score pour ExtraTreeClassifier: 0.6890107888468543\n",
      "\n",
      " Optimisation des hyperparam√®tres pour GradientBoostingClassifier...\n",
      " Meilleurs hyperparam√®tres pour GradientBoostingClassifier: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200}\n",
      " Meilleurs score pour GradientBoostingClassifier: 0.9302819812246041\n",
      "\n",
      " Optimisation des hyperparam√®tres pour HistGradientBoostingClassifier...\n",
      " Meilleurs hyperparam√®tres pour HistGradientBoostingClassifier: {'learning_rate': 0.2, 'max_depth': 10, 'max_iter': 200}\n",
      " Meilleurs score pour HistGradientBoostingClassifier: 0.9377970435757321\n",
      "\n",
      " Optimisation des hyperparam√®tres pour AdaBoostClassifier...\n",
      " Meilleurs hyperparam√®tres pour AdaBoostClassifier: {'learning_rate': 0.2, 'n_estimators': 300}\n",
      " Meilleurs score pour AdaBoostClassifier: 0.7395789547428891\n",
      "\n",
      " Optimisation des hyperparam√®tres pour CatBoostClassifier...\n",
      " Meilleurs hyperparam√®tres pour CatBoostClassifier: {'depth': 10, 'iterations': 300, 'learning_rate': 0.2}\n",
      " Meilleurs score pour CatBoostClassifier: 0.9248122460417543\n",
      "\n",
      "üìä R√©sum√© des meilleurs hyperparam√®tres :\n",
      "LGBMClassifier: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.2, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': None, 'num_leaves': 40, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "BaggingClassifier: {'bootstrap': True, 'bootstrap_features': False, 'estimator': None, 'max_features': 1.0, 'max_samples': 0.7, 'n_estimators': 200, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "RandomForestClassifier: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 5, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "DecisionTreeClassifier: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}\n",
      "ExtraTreeClassifier: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'random'}\n",
      "GradientBoostingClassifier: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.05, 'loss': 'log_loss', 'max_depth': 5, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 200, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "HistGradientBoostingClassifier: {'categorical_features': 'from_dtype', 'class_weight': None, 'early_stopping': 'auto', 'interaction_cst': None, 'l2_regularization': 0.0, 'learning_rate': 0.2, 'loss': 'log_loss', 'max_bins': 255, 'max_depth': 10, 'max_features': 1.0, 'max_iter': 200, 'max_leaf_nodes': 31, 'min_samples_leaf': 20, 'monotonic_cst': None, 'n_iter_no_change': 10, 'random_state': None, 'scoring': 'loss', 'tol': 1e-07, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "AdaBoostClassifier: {'algorithm': 'deprecated', 'estimator': None, 'learning_rate': 0.2, 'n_estimators': 300, 'random_state': None}\n",
      "CatBoostClassifier: {'verbose': 0, 'depth': 10, 'iterations': 300, 'learning_rate': 0.2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "best_models = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n Optimisation des hyperparam√®tres pour {model_name}...\")\n",
    "\n",
    "    param_grid = param_grids.get(model_name, None)\n",
    "    if param_grid:\n",
    "        search = GridSearchCV(model, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "        search.fit(X_train, y_train)\n",
    "        \n",
    "        # Sauvegarder le meilleur mod√®le\n",
    "        best_models[model_name] = search.best_estimator_\n",
    "        \n",
    "        print(f\" Meilleurs hyperparam√®tres pour {model_name}: {search.best_params_}\")\n",
    "        print(f\" Meilleurs score pour {model_name}: {search.best_score_}\")\n",
    "    else:\n",
    "        print(f\" Pas de grille d'hyperparam√®tres d√©finie pour {model_name}.\")\n",
    "\n",
    "print(\"\\nüìä R√©sum√© des meilleurs hyperparam√®tres :\")\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f\"{model_name}: {best_model.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Optimisation des hyperparam√®tres pour LGBMClassifier...\n",
      "[LightGBM] [Info] Number of positive: 1951, number of negative: 1951\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 3902, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      " Meilleurs hyperparam√®tres pour LGBMClassifier: {'learning_rate': 0.2, 'n_estimators': 500, 'num_leaves': 20}\n",
      " Meilleurs score pour LGBMClassifier: 0.9500305484932695\n",
      "\n",
      " Optimisation des hyperparam√®tres pour BaggingClassifier...\n",
      " Meilleurs hyperparam√®tres pour BaggingClassifier: {'bootstrap': True, 'max_samples': 1.0, 'n_estimators': 200}\n",
      " Meilleurs score pour BaggingClassifier: 0.9343964208991111\n",
      "\n",
      " Optimisation des hyperparam√®tres pour RandomForestClassifier...\n",
      " Meilleurs hyperparam√®tres pour RandomForestClassifier: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 300}\n",
      " Meilleurs score pour RandomForestClassifier: 0.91184887364749\n",
      "\n",
      " Optimisation des hyperparam√®tres pour DecisionTreeClassifier...\n",
      " Meilleurs hyperparam√®tres pour DecisionTreeClassifier: {'criterion': 'entropy', 'max_depth': 20, 'min_samples_split': 2}\n",
      " Meilleurs score pour DecisionTreeClassifier: 0.9026188927649343\n",
      "\n",
      " Optimisation des hyperparam√®tres pour ExtraTreeClassifier...\n",
      " Meilleurs hyperparam√®tres pour ExtraTreeClassifier: {'criterion': 'entropy', 'max_depth': None, 'min_samples_split': 10}\n",
      " Meilleurs score pour ExtraTreeClassifier: 0.7216912040048089\n",
      "\n",
      " Optimisation des hyperparam√®tres pour GradientBoostingClassifier...\n",
      " Meilleurs hyperparam√®tres pour GradientBoostingClassifier: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 300}\n",
      " Meilleurs score pour GradientBoostingClassifier: 0.9397790654131931\n",
      "\n",
      " Optimisation des hyperparam√®tres pour HistGradientBoostingClassifier...\n",
      " Meilleurs hyperparam√®tres pour HistGradientBoostingClassifier: {'learning_rate': 0.2, 'max_depth': 10, 'max_iter': 300}\n",
      " Meilleurs score pour HistGradientBoostingClassifier: 0.9454183172707386\n",
      "\n",
      " Optimisation des hyperparam√®tres pour AdaBoostClassifier...\n",
      " Meilleurs hyperparam√®tres pour AdaBoostClassifier: {'learning_rate': 0.2, 'n_estimators': 300}\n",
      " Meilleurs score pour AdaBoostClassifier: 0.7265488086087624\n",
      "\n",
      " Optimisation des hyperparam√®tres pour CatBoostClassifier...\n",
      " Meilleurs hyperparam√®tres pour CatBoostClassifier: {'depth': 10, 'iterations': 300, 'learning_rate': 0.2}\n",
      " Meilleurs score pour CatBoostClassifier: 0.9372191016772108\n",
      "\n",
      "üìä R√©sum√© des meilleurs hyperparam√®tres :\n",
      "LGBMClassifier: {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.2, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 500, 'n_jobs': None, 'num_leaves': 20, 'objective': None, 'random_state': None, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "BaggingClassifier: {'bootstrap': True, 'bootstrap_features': False, 'estimator': None, 'max_features': 1.0, 'max_samples': 1.0, 'n_estimators': 200, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "RandomForestClassifier: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 300, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "DecisionTreeClassifier: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': 20, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}\n",
      "ExtraTreeClassifier: {'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'entropy', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'random'}\n",
      "GradientBoostingClassifier: {'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 10, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 300, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "HistGradientBoostingClassifier: {'categorical_features': 'from_dtype', 'class_weight': None, 'early_stopping': 'auto', 'interaction_cst': None, 'l2_regularization': 0.0, 'learning_rate': 0.2, 'loss': 'log_loss', 'max_bins': 255, 'max_depth': 10, 'max_features': 1.0, 'max_iter': 300, 'max_leaf_nodes': 31, 'min_samples_leaf': 20, 'monotonic_cst': None, 'n_iter_no_change': 10, 'random_state': None, 'scoring': 'loss', 'tol': 1e-07, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "AdaBoostClassifier: {'algorithm': 'deprecated', 'estimator': None, 'learning_rate': 0.2, 'n_estimators': 300, 'random_state': None}\n",
      "CatBoostClassifier: {'verbose': 0, 'depth': 10, 'iterations': 300, 'learning_rate': 0.2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# üîπ Recherche des meilleurs hyperparam√®tres avec GridSearchCV\n",
    "best_models = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\n Optimisation des hyperparam√®tres pour {model_name}...\")\n",
    "\n",
    "    param_grid = param_grids.get(model_name, None)\n",
    "    if param_grid:\n",
    "        search = GridSearchCV(model, param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "        search.fit(X_res, y_res)\n",
    "        \n",
    "        # Sauvegarder le meilleur mod√®le\n",
    "        best_models[model_name] = search.best_estimator_\n",
    "        \n",
    "        print(f\" Meilleurs hyperparam√®tres pour {model_name}: {search.best_params_}\")\n",
    "        print(f\" Meilleurs score pour {model_name}: {search.best_score_}\")\n",
    "    else:\n",
    "        print(f\" Pas de grille d'hyperparam√®tres d√©finie pour {model_name}.\")\n",
    "\n",
    "# üîπ R√©sum√© des meilleurs hyperparam√®tres trouv√©s\n",
    "print(\"\\nüìä R√©sum√© des meilleurs hyperparam√®tres :\")\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f\"{model_name}: {best_model.get_params()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 1453, number of negative: 1473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001139 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3060\n",
      "[LightGBM] [Info] Number of data points in the train set: 2926, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.496582 -> initscore=-0.013671\n",
      "[LightGBM] [Info] Start training from score -0.013671\n",
      "Score du mod√®le Light: 0.9539\n"
     ]
    }
   ],
   "source": [
    "#Lightgbm\n",
    "model=LGBMClassifier(n_estimators=300, learning_rate= 0.2,  num_leaves= 20)\n",
    "\n",
    "\n",
    "#search = GridSearchCV(model, param_grid=param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Sauvegarder le meilleur mod√®le\n",
    "best_models[model_name] = search.best_estimator_\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Score du mod√®le Light: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du mod√®le Hist: 0.9508\n"
     ]
    }
   ],
   "source": [
    "#Hist\n",
    "\n",
    "\n",
    "#Lightgbm\n",
    "model=HistGradientBoostingClassifier( learning_rate= 0.2, max_depth= 10, max_iter =200)\n",
    "\n",
    "\n",
    "#search = GridSearchCV(model, param_grid=param_grid, cv=3, scoring=\"accuracy\", n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Sauvegarder le meilleur mod√®le\n",
    "best_models[model_name] = search.best_estimator_\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f\"Score du mod√®le Hist: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
